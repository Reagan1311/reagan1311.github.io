<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Learning Precise Affordances from Egocentric Videos for Robotic Manipulation">
  <meta name="keywords" content="Affordance learning, Egocentric videos, Human-object interaction, Robotic
  manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Aff-Grasp</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="../project_page_static/css/bulma.min.css">
  <link rel="stylesheet" href="../project_page_static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="../project_page_static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="../project_page_static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../project_page_static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="../project_page_static/js/fontawesome.all.min.js"></script>
  <script src="../project_page_static/js/bulma-carousel.min.js"></script>
  <script src="../project_page_static/js/bulma-slider.min.js"></script>
  <script src="../project_page_static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" target="_blank" href="https://reagan1311.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" target="_blank" href="https://reagan1311.github.io/locate/">
            LOCATE
          </a>
          <a class="navbar-item" target="_blank" href="https://reagan1311.github.io/ooal/">
            OOAL
          </a>
          <a class="navbar-item" target="_blank" href="https://reagan1311.github.io/mask2iv/">
            Mask2IV
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Learning Precise Affordances from Egocentric Videos for Robotic Manipulation</h1>
          <h3 class="title is-4 conference-authors"><a target="_blank" href="https://iccv.thecvf.com/">ICCV 2025</a></h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="http://reagan1311.github.io/">Gen Li</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://tsagkas.github.io/">Nikolaos Tsagkas</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://scholar.google.com/citations?user=9a1PjCIAAAAJ&hl=en">Jifei Song</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://www.edinburgh-robotics.org/students/ruaridh-mon-williams">Ruaridh Mon-Williams</a><sup>1</sup>,</span>
            <br><span class="author-block">
              <a target="_blank" href="https://homepages.inf.ed.ac.uk/svijayak/">Sethu Vijayakumar</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://shaokun91.github.io/">Kun Shao</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://laurasevilla.me/">Laura Sevilla-Lara</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Edinbugh,</span>
            <span class="author-block"><sup>2</sup>Huawei Noahâ€™s Ark Lab</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a target="_blank" href="paper/AffGrasp.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->

            <!-- Arxiv Link. -->
            <span class="link-block">
              <a target="_blank" href="https://arxiv.org/abs/2408.10123"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="ai ai-arxiv"></i>
                </span>
                <span>ArXiv</span>
              </a>
            </span>

            <!-- PDF Link. -->
            <span class="link-block">
              <a target="_blank" href="paper/09971_supp.pdf"
                  class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                </span>
                <span>Supp</span>
              </a>
            </span>

            <!-- Video Link. -->
            <span class="link-block">
              <a target="_blank" href="https://www.youtube.com/watch?v=zEXrEH3I3Pc"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
              </a>
            </span>

            <!-- Talk Link. -->
            <!-- <span class="link-block">
              <a target="_blank" href="https://youtu.be/QcuXwmQgurE"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-chalkboard-teacher"></i>
                </span>
                <span>Talk</span>
              </a>
            </span> -->


            <!-- Colab Link. -->
            <!-- <span class="link-block">
              <a target="_blank" href="https://colab.research.google.com/drive/1HAqemP4cE81SQ6QO1-N85j5bF4C0qLs0?usp=sharing"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fa fa-book" aria-hidden="true"></i>
                </span>
                <span>Colab</span>
                </a>
            </span> -->

            <!-- Code Link. -->
            <span class="link-block">
              <a target="_blank" href="https://github.com/Reagan1311/Aff-Grasp"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>

            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered is-centered">
          <!-- <video id="teaser" autoplay muted loop height="100%"> -->
            <!-- <source src="intro-gif.gif" -->
                    <!-- type="video/mp4"> -->
          <!-- </video> -->
          <img src="media/figures/intro-gif.gif" class="interpolation-image" 
          alt="Interpolate start reference image." />
        </div>
        <br>
        <h2 class="subtitle has-text-centered">
          We present a complete affordance learning system that encompasses data collection (from egocentric videos), 
          <br>effective model training, and robot deployment for manipulation tasks.
          <!-- <span class="dperact">PerAct</span> is an end-to-end behavior-cloning agent that can learn <br><b>a single language-conditioned policy</b> for 18 RLBench tasks with <b>249 unique task variations</b> -->
        </h2>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/stick.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/screw-handover.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/stir.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/cut-handover.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/pour.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<h2 class="subtitle has-text-centered">
  <br>
  Given a task and a cluttered scene, the robot can select the object that possesses the related affordance,
  grasp the correct part, <br>and apply the functional part to the target object to perform desired actions.
</h2>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
            <p>
              Affordance, defined as the potential actions that an object offers, is crucial for embodied AI agents.
              For example, such knowledge directs an agent to grasp a knife by the handle for cutting or by the blade for safe handover.
              While existing approaches have made notable progress, affordance research still faces three key challenges: data scarcity, poor generalization, and real-world deployment.
              Specifically, there is a lack of large-scale affordance datasets with precise segmentation maps, existing models struggle to generalize across different domains or novel object and affordance classes, and little work demonstrates deployability in real-world scenarios.
            </p>
            
            <p>            
              In this work, we address these issues by proposing a complete affordance learning system that
              (1) takes in egocentric videos and outputs precise affordance annotations without human labeling, 
              (2) leverages geometric information and vision foundation models to improve generalization, and 
              (3) introduces a framework that facilitates affordance-oriented robotic manipulation such as tool grasping and robot-to-human tool handover.
            </p>

            <p> 
              Experimental results show that our model surpasses the state-of-the-art by 13.8% in mIoU, and the framework achieves 77.1% successful grasping among 179 trials,
              including evaluations on seen, unseen classes, and cluttered scenes.
            </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>

</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">

    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <!-- <h2 class="title is-3"><span class="dperact">PerAct</span></h2> -->

        <!-- Interpolating. -->
        <h3 class="title is-4">Geometry-guided Affordance Transformer (GAT)</h3>
        <img src="media/figures/model-fig.png" class="interpolation-image" width="90%" style="display: block; margin: auto;"
        alt="Interpolate start reference image." />
        <br>
        <p>
          The architecture of GAT. It consists of a DINOv2 image encoder, a depth feature injector, an embedder, and LoRA layers. The model performs segmentation by computing cosine similarity between upsampled features and learnable / CLIP text embeddings.
        </p>
        <br>
        <br>
        <h3 class="title is-4">Aff-Grasp</h3>  
        <img src="media/figures/grasp-pipeline.png" class="interpolation-image" width="80%" style="display: block; margin: auto;"
        alt="Interpolate start reference image." />
        <br>
          <p>
            The framework of Aff-Grasp. It first employs an open-vocabulary detector to locate all objects within the scene, which are then sent to GAT to determine if they possess the corresponding affordance required for the task. Afterwards, a 6 DoF grasp generation model, Contact-GraspNet, leverages the object's graspable affordance and the depth map to generate dense grasp proposals. Finally, the robot executes affordance-specific sequential motion primitives to apply the functional part to the target.
          </p>
        <br>
        <br>
        <!-- <h3 class="title is-4"></h3> -->
          <!-- <p class="justify">
             The input grid is 100&times;100&times;100 = 1 million voxels. After extracting 5&times;5&times;5 patches, the input is 20&times;20&times;20 = 8000 embeddings long. Despite this long sequence, Perceiver uses a small set of latent vectors to encode the input. These latent vectors are randomly initialized and trained end-to-end. This approach decouples the depth of the Transformer self-attention layers from the dimensionality of the input space, which allows us train <span class="dperact">PerAct</span> on very large input voxel grids. Perceiver has been deployed in several domains like <a target="_blank" href="https://www.deepmind.com/publications/perceiver-ar-general-purpose-long-context-autoregressive-generation">long-context auto-regressive generation</a>, <a target="_blank" href="https://arxiv.org/abs/2204.14198">vision-language models for few-shot learning</a>, <a target="_blank" href="https://arxiv.org/abs/2107.14795">image and audio classification, and optical flow prediction.</a>
          </p> -->
        <!-- <br/> -->
        <!-- <br/> -->
                <!-- Paper video. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-two-thirds">
            <h2 class="title is-3">Video for Robot Experiments</h2>
            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/zEXrEH3I3Pc?rel=0&amp;showinfo=0"
                      frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              <!-- <iframe src="https://www.youtube.com/embed/PqBYFl3gmyY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> -->
              <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/zEXrEH3I3Pc?si=jvNwoxk4DZ2EEdCF" title="YouTube video player"  -->
                <!-- frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe> -->
            </div>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{li2025affgrasp,
      title     = {Learning Precise Affordances from Egocentric Videos for Robotic Manipulation}, 
      author    = {Li, Gen and Tsagkas, Nikolaos and Song, Jifei and Mon-Williams, Ruaridh and Vijayakumar, Sethu and Shao, Kun and Sevilla-Lara, Laura},
      journal   = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
      year      = {2025},
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a href="https://peract.github.io/">PerAct</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
